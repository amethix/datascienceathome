---
title: Parallelizing and distributing Stochastic Gradient Descent
layout: post
summary: Continuing the discussion of the last two episodes, there is one more aspect
  of deep learning that I would love to consider and therefore left as a full episode,
  that is parallelising and distributing deep learning on relatively large clusters.
comments: true
---

## Our Site has moved!
### The new web address for this page is
### [https://amethix.com/2018/10/parallelizing-and-distributing-stochastic-gradient-descent/](https://amethix.com/2018/10/parallelizing-and-distributing-stochastic-gradient-descent/)
### See you there!
